# VLA_PAPERS

## 算法

| 项目简称       | 论文题目 (ArXiv 链接)                                                                                  | GitHub 代码链接                                              |
|---------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| **VoxPoser**  | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://arxiv.org/abs/2307.05973)  | https://github.com/huangwl18/VoxPoser                        |
| **ReKEP**     | [ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://arxiv.org/abs/2409.01652)  | https://github.com/rekep-robot/ReKep                        |
| **AnyGrasp**  | [AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains](https://arxiv.org/abs/2212.08333) | https://github.com/graspnet/anygrasp_sdk                    |
| **ACT**       | [Learning Fine‑Grained Bimanual Manipulation with Low‑Cost Hardware](https://arxiv.org/abs/2304.13705) | https://github.com/tonyzhaozh/act                            |
| **Mobile ALOHA** | [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low‑Cost Whole‑Body Teleoperation](https://arxiv.org/abs/2401.02117) | https://github.com/MarkFzp/mobile-aloha                     |
| **RoboAgent** (MT‑ACT) | [RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking](https://arxiv.org/abs/2309.01918)  | https://github.com/robopen/roboagent                        |
| **Diffusion Policy (DP)** | [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/abs/2303.04137)  | https://github.com/real-stanford/diffusion_policy           |
| **3DP (DP3)** | [3D Diffusion Policy: Generalizable Visuomotor Control with 3D Representations](https://arxiv.org/abs/2403.03954) | https://github.com/YanjieZe/3D-Diffusion-Policy              |
| **OpenVLA**   | [OpenVLA: An Open‑Source Vision‑Language‑Action Model](https://arxiv.org/abs/2406.09246) | https://github.com/openvla/openvla                           |
| **OpenVLA‑OFT** | [Fine‑Tuning Vision‑Language‑Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645)| https://openvla-oft.github.io/                             |
| **π₀**        | [A Vision‑Language‑Action Flow Model for General Robot Control (π₀)](https://arxiv.org/abs/2410.24164) | https://github.com/lucidrains/pi-zero-pytorch               |
| **π₀‑FAST**   | [Efficient Action Tokenization for Vision‑Language‑Action Models (FAST)](https://arxiv.org/abs/2501.09747)  | https://github.com/Physical-Intelligence/openpi              |
| **RDT‑1B**    | [RDT‑1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/abs/2410.07864) | https://github.com/robotics-diffusion-transformer/rdt-1b    |
| **CLIPort**     | [CLIPort: What and Where Pathways for Robotic Manipulation](https://arxiv.org/abs/2107.03389)                                | https://github.com/harvard-robotics/cliport                    |
| **Transporter** | [Transporter Networks: Rearranging the Visual World for Robotic Manipulation](https://arxiv.org/abs/2011.12917)               | https://github.com/google-research/google-research/tree/master/transporter |
| **SERL**           | [SERL: A Software Suite for Sample‑Efficient Robotic Reinforcement Learning](https://arxiv.org/abs/2401.16013)               | https://github.com/serl-robot/serl                           |
| **NVIDIA‑GOOT‑T1** | [Isaac GR00T N1: An Open Foundation Model for Humanoid Robots](https://nvidianews.nvidia.com/news/nvidia-isaac-gr00t-n1-open-humanoid-robot-foundation-model-simulation-frameworks) | https://github.com/NVIDIA-Omniverse/IsaacSDK                 |
| **HI‑Robotic**     | [Hi Robot: Open‑Ended Instruction Following with Hierarchical Vision‑Language‑Action Models](https://arxiv.org/abs/2502.19417) | https://github.com/hierarchical-robot/hi-robotic             |

## 数据集 / Benchmark

|--------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| **OpenX**          | [Open X-Embodiment: Robotic Learning Datasets and RT‑X Models](https://arxiv.org/abs/2310.08864)  | https://robotics-transformer-x.github.io/ |
| **RoboMIND**       | [RoboMIND: Benchmark on Multi-embodiment Intelligence](https://arxiv.org/abs/2412.13877)  | https://x-humanoid-robomind.github.io/  |
| **Libero**         | [LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning](https://arxiv.org/abs/2306.03310) | https://github.com/Lifelong-Robot-Learning/LIBERO |
| **RoboVerse**      | [ROBOVERSE: A Unified Benchmark for Scalable and Generalizable Vision-Language Robotic Manipulation (CVPR 2024 Draft)](https://embodied-ai.org/papers/2024/16_RoboVerse_A_Unified_Simulat.pdf) | https://github.com/RoboVerseOrg/RoboVerse :contentReference |
| **CALVIN**         | [CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks](https://arxiv.org/abs/2112.03227) | https://github.com/mees/calvin    |








> *注：GitHub 链接如未找到官方仓库，则列出社区复刻或项目主页。欢迎补充或纠正。*
