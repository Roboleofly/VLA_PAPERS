# VLA_PAPERS

## 算法

| 项目简称       | 论文题目 (ArXiv 链接)                                                                                  | GitHub 代码链接                                              |
|---------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| **VoxPoser**  | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://arxiv.org/abs/2307.05973)  | https://github.com/huangwl18/VoxPoser                        |
| **ReKEP**     | [ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://arxiv.org/abs/2409.01652)  | https://github.com/rekep-robot/ReKep                        |
| **AnyGrasp**  | [AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains](https://arxiv.org/abs/2212.08333) | https://github.com/graspnet/anygrasp_sdk                    |
| **ACT**       | [Learning Fine‑Grained Bimanual Manipulation with Low‑Cost Hardware](https://arxiv.org/abs/2304.13705) | https://github.com/tonyzhaozh/act                            |
| **Mobile ALOHA** | [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low‑Cost Whole‑Body Teleoperation](https://arxiv.org/abs/2401.02117) | https://github.com/MarkFzp/mobile-aloha                     |
| **RoboAgent** (MT‑ACT) | [RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking](https://arxiv.org/abs/2309.01918)  | https://github.com/robopen/roboagent                        |
| **Diffusion Policy (DP)** | [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/abs/2303.04137)  | https://github.com/real-stanford/diffusion_policy           |
| **3DP (DP3)** | [3D Diffusion Policy: Generalizable Visuomotor Control with 3D Representations](https://arxiv.org/abs/2403.03954) | https://github.com/YanjieZe/3D-Diffusion-Policy              |
| **OpenVLA**   | [OpenVLA: An Open‑Source Vision‑Language‑Action Model](https://arxiv.org/abs/2406.09246) | https://github.com/openvla/openvla                           |
| **OpenVLA‑OFT** | [Fine‑Tuning Vision‑Language‑Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645)| https://openvla-oft.github.io/                             |
| **π₀**        | [A Vision‑Language‑Action Flow Model for General Robot Control (π₀)](https://arxiv.org/abs/2410.24164) | https://github.com/Physical-Intelligence/openpi              |
| **π₀‑FAST**   | [Efficient Action Tokenization for Vision‑Language‑Action Models (FAST)](https://arxiv.org/abs/2501.09747)  | https://github.com/Physical-Intelligence/openpi              |
|**π0.5**|[π0.5: a Vision-Language-Action Model with Open-World Generalization](https://www.physicalintelligence.company/download/pi05.pdf)||
| **RDT‑1B**    | [RDT‑1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/abs/2410.07864) | https://github.com/robotics-diffusion-transformer/rdt-1b    |
| **CLIPort**     | [CLIPort: What and Where Pathways for Robotic Manipulation](https://arxiv.org/abs/2107.03389)                                | https://github.com/harvard-robotics/cliport                    |
| **Transporter** | [Transporter Networks: Rearranging the Visual World for Robotic Manipulation](https://arxiv.org/abs/2011.12917)               | https://github.com/google-research/google-research/tree/master/transporter |
| **SERL**           | [SERL: A Software Suite for Sample‑Efficient Robotic Reinforcement Learning](https://arxiv.org/abs/2401.16013)               | https://github.com/serl-robot/serl                           |
| **BRS** | [BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities](https://arxiv.org/pdf/2503.05652) |  https://github.com/behavior-robot-suite/brs-algo   |
| **NVIDIA‑GOOT‑T1** | [Isaac GR00T N1: An Open Foundation Model for Humanoid Robots](https://arxiv.org/pdf/2503.14734) | https://github.com/NVIDIA/Isaac-GR00T     |
| **HiRT**     | [HiRT: Enhacing Robotic Control with Hierarchical Robot Transformers](https://arxiv.org/pdf/2410.05273) |        |
| **Gemini Robotics**     | [Gemini Robotics: Bring AI into the Physical World](https://arxiv.org/abs/2503.20020) |        |
| **ConRFT** | [ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy](https://arxiv.org/pdf/2502.05450) |https://github.com/cccedric/conrft|
| **DYNA-1** | [Dynamism v1 (DYNA-1) Model: A Breakthrough in Performance and Production-Ready Embodied AI](https://www.dyna.co/research) | |
|**Video Prediction Policy**| [Video Prediction Policy A Generalist Robot Policy with Predictive Visual Representations](https://arxiv.org/pdf/2412.14803)| https://github.com/roboterax/video-prediction-policy |

## 数据集 / Benchmark

| 项目简称       | 论文题目 (ArXiv 链接)                                                                                  | GitHub 代码链接                                              |
|---------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| **OpenX**          | [Open X-Embodiment: Robotic Learning Datasets and RT‑X Models](https://arxiv.org/abs/2310.08864)  | https://robotics-transformer-x.github.io/ |
| **RoboMIND**       | [RoboMIND: Benchmark on Multi-embodiment Intelligence](https://arxiv.org/abs/2412.13877)  | https://x-humanoid-robomind.github.io/  |
| **Libero**         | [LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning](https://arxiv.org/abs/2306.03310) | https://github.com/Lifelong-Robot-Learning/LIBERO |
| **RoboVerse**      | [ROBOVERSE: A Unified Benchmark for Scalable and Generalizable Vision-Language Robotic Manipulation (CVPR 2024 Draft)](https://embodied-ai.org/papers/2024/16_RoboVerse_A_Unified_Simulat.pdf) | https://github.com/RoboVerseOrg/RoboVerse |
| **CALVIN**         | [CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks](https://arxiv.org/abs/2112.03227) | https://github.com/mees/calvin    |
| **RoboTwin** | [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twin](https://arxiv.org/pdf/2504.13059) | https://github.com/TianxingChen/RoboTwin |
| **DROID** |[DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset](https://arxiv.org/abs/2403.12945) | https://github.com/droid-dataset/droid_policy_learning |








> *持续跟新中...*
